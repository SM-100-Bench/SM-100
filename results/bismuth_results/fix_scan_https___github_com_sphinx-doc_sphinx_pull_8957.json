{"scanned_subsystems":[{"name":"Utilities","files":["sphinx/util/__init__.py","sphinx/util/build_phase.py","sphinx/util/cfamily.py","sphinx/util/compat.py","sphinx/util/console.py","sphinx/util/docfields.py","sphinx/util/docstrings.py","sphinx/util/docutils.py","sphinx/util/fileutil.py","sphinx/util/i18n.py","sphinx/util/images.py","sphinx/util/inspect.py","sphinx/util/inventory.py","sphinx/util/jsdump.py","sphinx/util/jsonimpl.py","sphinx/util/logging.py","sphinx/util/matching.py","sphinx/util/math.py","sphinx/util/nodes.py","sphinx/util/osutil.py","sphinx/util/parallel.py","sphinx/util/png.py","sphinx/util/pycompat.py","sphinx/util/requests.py","sphinx/util/rst.py","sphinx/util/smartypants.py","sphinx/util/stemmer/__init__.py","sphinx/util/stemmer/porter.py","sphinx/util/tags.py","sphinx/util/template.py","sphinx/util/texescape.py","sphinx/util/typing.py"]}],"changesets":[{"title":"Fix Memory Management Issues in Sphinx Utilities","body":"## Overview\nThis pull request addresses several memory management issues across different utility modules in Sphinx, implementing size limits and LRU (Least Recently Used) caching strategies to prevent unbounded memory growth.\n\n### Changes Implemented\n1. **Pattern Matching Cache (`sphinx/util/matching.py`)**:\n   - Added a maximum size limit of 100 for the `_pat_cache`\n   - Implemented LRU eviction policy using `OrderedDict`\n   - Prevents unbounded growth of compiled regex pattern cache\n   - Added comprehensive tests to verify cache behavior\n\n2. **Logging System Memory Management (`sphinx/util/logging.py`)**:\n   - Added capacity limits to:\n     - `MemoryHandler`: Limits log record storage\n     - `OnceFilter`: Limits unique message tracking\n     - `LastMessagesWriter`: Limits message log size\n   - Implemented LRU-like eviction strategies\n   - Used `OrderedDict` and `deque` for efficient memory management\n   - Added new test cases to verify memory limit enforcement\n\n3. **File Handling Improvements (`sphinx/util/osutil.py`)**:\n   - Enhanced `FileAvoidWrite` to handle empty files more gracefully\n   - Improved `getvalue()` method to handle edge cases\n   - Added comprehensive test coverage for file writing scenarios\n\n### Key Benefits\n- Prevents potential memory leaks in long-running Sphinx applications\n- Ensures predictable memory usage across utility components\n- Maintains existing functionality while adding memory management safeguards\n\n### Testing\n- Added new test suites for each modified utility\n- Verified LRU cache eviction\n- Tested memory limit enforcement\n- Ensured no regressions in existing functionality\n\n### Recommended Review Focus\n- Memory management implementation\n- Edge case handling\n- Performance implications of new caching strategies\n\nFixes potential memory-related issues in Sphinx's utility modules.","commits":[{"message":"Fix the unbounded growth issue in the _pat_cache dictionary in sphinx/util/matching.py by implementing a size limit and an LRU (Least Recently Used) eviction policy to prevent potential memory issues in long-running applications.","diff":"diff --git a/sphinx/util/matching.py b/sphinx/util/matching.py\nindex d33ae03..4e3fe7a 100644\n--- a/sphinx/util/matching.py\n+++ b/sphinx/util/matching.py\n@@ -9,6 +9,7 @@\n \"\"\"\n \n import re\n+from collections import OrderedDict\n from typing import Callable, Dict, Iterable, List, Match, Pattern\n \n from sphinx.util.osutil import canon_path\n@@ -86,13 +87,21 @@ class Matcher:\n DOTFILES = Matcher(['**/.*'])\n \n \n-_pat_cache = {}  # type: Dict[str, Pattern]\n+# LRU cache for compiled patterns with a maximum size to prevent unbounded growth\n+_MAX_PAT_CACHE_SIZE = 100\n+_pat_cache = OrderedDict()  # type: OrderedDict[str, Pattern]\n \n \n def patmatch(name: str, pat: str) -> Match[str]:\n     \"\"\"Return if name matches pat.  Adapted from fnmatch module.\"\"\"\n     if pat not in _pat_cache:\n+        # If cache is full, remove the least recently used item (first item in OrderedDict)\n+        if len(_pat_cache) >= _MAX_PAT_CACHE_SIZE:\n+            _pat_cache.popitem(last=False)\n         _pat_cache[pat] = re.compile(_translate_pattern(pat))\n+    else:\n+        # Move pattern to the end of the OrderedDict (mark as most recently used)\n+        _pat_cache.move_to_end(pat)\n     return _pat_cache[pat].match(name)\n \n \n@@ -102,6 +111,12 @@ def patfilter(names: Iterable[str], pat: str) -> List[str]:\n     Adapted from fnmatch module.\n     \"\"\"\n     if pat not in _pat_cache:\n+        # If cache is full, remove the least recently used item (first item in OrderedDict)\n+        if len(_pat_cache) >= _MAX_PAT_CACHE_SIZE:\n+            _pat_cache.popitem(last=False)\n         _pat_cache[pat] = re.compile(_translate_pattern(pat))\n+    else:\n+        # Move pattern to the end of the OrderedDict (mark as most recently used)\n+        _pat_cache.move_to_end(pat)\n     match = _pat_cache[pat].match\n     return list(filter(match, names))\ndiff --git a/tests/test_util_matching_lru.py b/tests/test_util_matching_lru.py\nnew file mode 100644\nindex 0000000..faa2beb\n--- /dev/null\n+++ b/tests/test_util_matching_lru.py\n@@ -0,0 +1,77 @@\n+\"\"\"\n+    test_util_matching_lru\n+    ~~~~~~~~~~~~~~~~~~~~~~\n+\n+    Tests for the LRU cache implementation in sphinx.util.matching.\n+\n+    :copyright: Copyright 2007-2021 by the Sphinx team, see AUTHORS.\n+    :license: BSD, see LICENSE for details.\n+\"\"\"\n+\n+from sphinx.util.matching import patmatch, patfilter, _pat_cache, _MAX_PAT_CACHE_SIZE\n+\n+\n+def test_pat_cache_size_limit():\n+    \"\"\"Test that the pattern cache size is limited to _MAX_PAT_CACHE_SIZE.\"\"\"\n+    # Clear the cache first\n+    _pat_cache.clear()\n+    \n+    # Add more patterns than the cache size limit\n+    for i in range(_MAX_PAT_CACHE_SIZE + 10):\n+        pattern = f\"pattern{i}.*\"\n+        patmatch(f\"pattern{i}.txt\", pattern)\n+    \n+    # Check that the cache size is limited\n+    assert len(_pat_cache) <= _MAX_PAT_CACHE_SIZE\n+\n+\n+def test_pat_cache_lru_eviction():\n+    \"\"\"Test that the least recently used patterns are evicted from the cache.\"\"\"\n+    # Clear the cache first\n+    _pat_cache.clear()\n+    \n+    # Fill the cache\n+    for i in range(_MAX_PAT_CACHE_SIZE):\n+        pattern = f\"pattern{i}.*\"\n+        patmatch(f\"pattern{i}.txt\", pattern)\n+    \n+    # All these patterns should be in the cache\n+    for i in range(_MAX_PAT_CACHE_SIZE):\n+        assert f\"pattern{i}.*\" in _pat_cache\n+    \n+    # Access a pattern to mark it as recently used\n+    patmatch(\"pattern0.txt\", \"pattern0.*\")\n+    \n+    # Add a new pattern, which should evict the least recently used pattern (pattern1.*)\n+    patmatch(f\"patternNew.txt\", \"patternNew.*\")\n+    \n+    # Check that pattern1.* was evicted (not pattern0.* which was recently used)\n+    assert \"pattern0.*\" in _pat_cache\n+    assert \"patternNew.*\" in _pat_cache\n+    assert \"pattern1.*\" not in _pat_cache\n+\n+\n+def test_patfilter_uses_lru_cache():\n+    \"\"\"Test that patfilter also uses the LRU cache mechanism.\"\"\"\n+    # Clear the cache first\n+    _pat_cache.clear()\n+    \n+    # Fill the cache using patfilter\n+    for i in range(_MAX_PAT_CACHE_SIZE):\n+        pattern = f\"pattern{i}.*\"\n+        patfilter([f\"pattern{i}.txt\"], pattern)\n+    \n+    # All these patterns should be in the cache\n+    for i in range(_MAX_PAT_CACHE_SIZE):\n+        assert f\"pattern{i}.*\" in _pat_cache\n+    \n+    # Access a pattern with patmatch to mark it as recently used\n+    patmatch(\"pattern0.txt\", \"pattern0.*\")\n+    \n+    # Add a new pattern with patfilter, which should evict the least recently used pattern\n+    patfilter([f\"patternNew.txt\"], \"patternNew.*\")\n+    \n+    # Check that pattern1.* was evicted (not pattern0.* which was recently used)\n+    assert \"pattern0.*\" in _pat_cache\n+    assert \"patternNew.*\" in _pat_cache\n+    assert \"pattern1.*\" not in _pat_cache\n\\ No newline at end of file"},{"message":"Fix the memory issues in the logging system by implementing size limits for the LastMessagesWriter, MemoryHandler, and OnceFilter classes in sphinx/util/logging.py to prevent unbounded memory growth.","diff":"diff --git a/sphinx/util/logging.py b/sphinx/util/logging.py\nindex 0978072..a5c8d78 100644\n--- a/sphinx/util/logging.py\n+++ b/sphinx/util/logging.py\n@@ -10,7 +10,7 @@\n \n import logging\n import logging.handlers\n-from collections import defaultdict\n+from collections import defaultdict, deque\n from contextlib import contextmanager\n from typing import IO, Any, Dict, Generator, List, Tuple, Union\n \n@@ -166,11 +166,19 @@ class NewLineStreamHandler(logging.StreamHandler):\n class MemoryHandler(logging.handlers.BufferingHandler):\n     \"\"\"Handler buffering all logs.\"\"\"\n \n-    def __init__(self) -> None:\n-        super().__init__(-1)\n+    def __init__(self, capacity: int = 1000) -> None:\n+        \"\"\"Initialize the handler with the buffer size.\n+        \n+        :param capacity: Maximum number of records to store (default: 1000)\n+        \"\"\"\n+        super().__init__(capacity)\n \n     def shouldFlush(self, record: logging.LogRecord) -> bool:\n-        return False  # never flush\n+        if len(self.buffer) >= self.capacity:\n+            # Remove oldest records when capacity is reached\n+            self.buffer = self.buffer[-(self.capacity-1):] + [record]\n+            return False\n+        return False  # never flush automatically\n \n     def flushTo(self, logger: logging.Logger) -> None:\n         self.acquire()\n@@ -448,15 +456,28 @@ class MessagePrefixFilter(logging.Filter):\n class OnceFilter(logging.Filter):\n     \"\"\"Show the message only once.\"\"\"\n \n-    def __init__(self, name: str = '') -> None:\n+    def __init__(self, name: str = '', capacity: int = 500) -> None:\n+        \"\"\"Initialize the filter with a maximum capacity.\n+        \n+        :param name: Name of the filter\n+        :param capacity: Maximum number of unique messages to store (default: 500)\n+        \"\"\"\n         super().__init__(name)\n         self.messages = {}  # type: Dict[str, List]\n+        self.capacity = capacity\n \n     def filter(self, record: logging.LogRecord) -> bool:\n         once = getattr(record, 'once', '')\n         if not once:\n             return True\n         else:\n+            # Check if we've reached capacity and need to remove old entries\n+            if len(self.messages) >= self.capacity:\n+                # Remove an arbitrary item (first one in iteration)\n+                if self.messages:\n+                    key = next(iter(self.messages))\n+                    self.messages.pop(key)\n+                    \n             params = self.messages.setdefault(record.msg, [])\n             if record.args in params:\n                 return False\n@@ -555,8 +576,20 @@ class SafeEncodingWriter:\n \n class LastMessagesWriter:\n     \"\"\"Stream writer which memories last 10 messages to save trackback\"\"\"\n-    def __init__(self, app: \"Sphinx\", stream: IO) -> None:\n+    def __init__(self, app: \"Sphinx\", stream: IO, capacity: int = 10) -> None:\n+        \"\"\"Initialize the writer with a maximum capacity.\n+        \n+        :param app: Sphinx application instance\n+        :param stream: Output stream\n+        :param capacity: Maximum number of messages to store (default: 10)\n+        \"\"\"\n         self.app = app\n+        # Initialize messagelog as a deque with max length if it doesn't exist\n+        if not hasattr(app, 'messagelog'):\n+            app.messagelog = deque(maxlen=capacity)\n+        elif not isinstance(app.messagelog, deque):\n+            # Convert existing messagelog to deque with maxlen if it's not already\n+            app.messagelog = deque(app.messagelog, maxlen=capacity)\n \n     def write(self, data: str) -> None:\n         self.app.messagelog.append(data)"},{"message":"Fix the type annotation issue in the get_node_location function in sphinx/util/logging.py by changing the return type from 'str' to 'Optional[str]' to accurately reflect that the function can return None.","diff":"diff --git a/sphinx/util/logging.py b/sphinx/util/logging.py\nindex a5c8d78..85d2d8d 100644\n--- a/sphinx/util/logging.py\n+++ b/sphinx/util/logging.py\n@@ -12,7 +12,7 @@ import logging\n import logging.handlers\n from collections import defaultdict, deque\n from contextlib import contextmanager\n-from typing import IO, Any, Dict, Generator, List, Tuple, Union\n+from typing import IO, Any, Dict, Generator, List, Optional, Tuple, Union\n \n from docutils import nodes\n from docutils.nodes import Node\n@@ -530,7 +530,7 @@ class WarningLogRecordTranslator(SphinxLogRecordTranslator):\n     LogRecordClass = SphinxWarningLogRecord\n \n \n-def get_node_location(node: Node) -> str:\n+def get_node_location(node: Node) -> Optional[str]:\n     (source, line) = get_source_line(node)\n     if source and line:\n         return \"%s:%s\" % (source, line)"},{"message":"Analyze the OnceFilter class in sphinx/util/logging.py to determine if it properly limits memory usage, and if not, implement a fix to prevent unbounded memory growth by limiting the number of stored messages.","diff":"diff --git a/sphinx/util/logging.py b/sphinx/util/logging.py\nindex 85d2d8d..48c7df1 100644\n--- a/sphinx/util/logging.py\n+++ b/sphinx/util/logging.py\n@@ -10,7 +10,7 @@\n \n import logging\n import logging.handlers\n-from collections import defaultdict, deque\n+from collections import defaultdict, deque, OrderedDict\n from contextlib import contextmanager\n from typing import IO, Any, Dict, Generator, List, Optional, Tuple, Union\n \n@@ -463,7 +463,7 @@ class OnceFilter(logging.Filter):\n         :param capacity: Maximum number of unique messages to store (default: 500)\n         \"\"\"\n         super().__init__(name)\n-        self.messages = {}  # type: Dict[str, List]\n+        self.messages = OrderedDict()  # type: OrderedDict[str, List]\n         self.capacity = capacity\n \n     def filter(self, record: logging.LogRecord) -> bool:\n@@ -471,18 +471,28 @@ class OnceFilter(logging.Filter):\n         if not once:\n             return True\n         else:\n-            # Check if we've reached capacity and need to remove old entries\n-            if len(self.messages) >= self.capacity:\n-                # Remove an arbitrary item (first one in iteration)\n-                if self.messages:\n-                    key = next(iter(self.messages))\n-                    self.messages.pop(key)\n-                    \n-            params = self.messages.setdefault(record.msg, [])\n-            if record.args in params:\n-                return False\n-\n-            params.append(record.args)\n+            # Check if message is already in the cache\n+            if record.msg in self.messages:\n+                params = self.messages[record.msg]\n+                # If this exact message with these args was already logged, filter it out\n+                if record.args in params:\n+                    # Move to end of OrderedDict to mark as recently used\n+                    params_copy = params.copy()\n+                    self.messages.pop(record.msg)\n+                    self.messages[record.msg] = params_copy\n+                    return False\n+                # New args for existing message, add them and move to end\n+                params.append(record.args)\n+                # Move to end of OrderedDict to mark as recently used\n+                self.messages.move_to_end(record.msg)\n+            else:\n+                # New message, check capacity first\n+                if len(self.messages) >= self.capacity:\n+                    # Remove oldest item (first one in OrderedDict)\n+                    self.messages.popitem(last=False)\n+                # Add new message with its args\n+                self.messages[record.msg] = [record.args]\n+                \n             return True\n \n \ndiff --git a/tests/test_util/test_logging_oncefilter.py b/tests/test_util/test_logging_oncefilter.py\nnew file mode 100644\nindex 0000000..d795513\n--- /dev/null\n+++ b/tests/test_util/test_logging_oncefilter.py\n@@ -0,0 +1,132 @@\n+\"\"\"\n+    Test for sphinx.util.logging.OnceFilter\n+\"\"\"\n+\n+import unittest\n+import logging\n+from sphinx.util.logging import OnceFilter\n+\n+\n+class TestOnceFilter(unittest.TestCase):\n+    def test_capacity_limit(self):\n+        \"\"\"Test that OnceFilter properly limits the number of stored messages.\"\"\"\n+        # Create a filter with small capacity for testing\n+        capacity = 5\n+        filter = OnceFilter(capacity=capacity)\n+        \n+        # Create mock log records that inherit from LogRecord\n+        class MockRecord(logging.LogRecord):\n+            def __init__(self, msg, args=None):\n+                if args is None:\n+                    args = ()\n+                super().__init__(\n+                    name=\"test\",\n+                    level=logging.INFO,\n+                    pathname=\"\",\n+                    lineno=0,\n+                    msg=msg,\n+                    args=args,\n+                    exc_info=None\n+                )\n+                self.once = True\n+        \n+        # Add more messages than capacity\n+        for i in range(capacity + 3):\n+            record = MockRecord(f\"Message {i}\")\n+            filter.filter(record)\n+            \n+        # Check that dictionary size is limited to capacity\n+        self.assertLessEqual(len(filter.messages), capacity)\n+        \n+        # Check that oldest messages were removed (first ones should be gone)\n+        self.assertNotIn(\"Message 0\", filter.messages)\n+        self.assertNotIn(\"Message 1\", filter.messages)\n+        self.assertNotIn(\"Message 2\", filter.messages)\n+        \n+        # Check that newest messages are still there\n+        self.assertIn(f\"Message {capacity}\", filter.messages)\n+        self.assertIn(f\"Message {capacity+1}\", filter.messages)\n+        self.assertIn(f\"Message {capacity+2}\", filter.messages)\n+    \n+    def test_duplicate_messages(self):\n+        \"\"\"Test that OnceFilter correctly handles duplicate messages.\"\"\"\n+        filter = OnceFilter(capacity=10)\n+        \n+        # Create mock log records that inherit from LogRecord\n+        class MockRecord(logging.LogRecord):\n+            def __init__(self, msg, args=None):\n+                if args is None:\n+                    args = ()\n+                super().__init__(\n+                    name=\"test\",\n+                    level=logging.INFO,\n+                    pathname=\"\",\n+                    lineno=0,\n+                    msg=msg,\n+                    args=args,\n+                    exc_info=None\n+                )\n+                self.once = True\n+        \n+        # First occurrence should pass through\n+        record1 = MockRecord(\"Duplicate message\", (1,))\n+        self.assertTrue(filter.filter(record1))\n+        \n+        # Second occurrence with same args should be filtered out\n+        record2 = MockRecord(\"Duplicate message\", (1,))\n+        self.assertFalse(filter.filter(record2))\n+        \n+        # Different args should pass through\n+        record3 = MockRecord(\"Duplicate message\", (2,))\n+        self.assertTrue(filter.filter(record3))\n+    \n+    def test_lru_behavior(self):\n+        \"\"\"Test that OnceFilter implements LRU (Least Recently Used) behavior.\"\"\"\n+        capacity = 3\n+        filter = OnceFilter(capacity=capacity)\n+        \n+        # Create mock log records that inherit from LogRecord\n+        class MockRecord(logging.LogRecord):\n+            def __init__(self, msg, args=None):\n+                if args is None:\n+                    args = ()\n+                super().__init__(\n+                    name=\"test\",\n+                    level=logging.INFO,\n+                    pathname=\"\",\n+                    lineno=0,\n+                    msg=msg,\n+                    args=args,\n+                    exc_info=None\n+                )\n+                self.once = True\n+        \n+        # Add initial messages to fill the cache\n+        for i in range(capacity):\n+            record = MockRecord(f\"Message {i}\")\n+            filter.filter(record)\n+        \n+        # Access the first message again to make it most recently used\n+        record_first = MockRecord(\"Message 0\")\n+        # This should return False since it's already in the cache\n+        self.assertFalse(filter.filter(record_first))\n+        \n+        # Add a new message that should evict the second message (now the oldest)\n+        record_new = MockRecord(\"Message new\")\n+        filter.filter(record_new)\n+        \n+        # Check that the second message was evicted\n+        self.assertNotIn(\"Message 1\", filter.messages)\n+        \n+        # Check that the first message is still there (because it was recently accessed)\n+        self.assertIn(\"Message 0\", filter.messages)\n+        \n+        # Check that the third message is still there\n+        self.assertIn(\"Message 2\", filter.messages)\n+        \n+        # Check that the new message is there\n+        self.assertIn(\"Message new\", filter.messages)\n+\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\\ No newline at end of file"},{"message":"Analyze the FileAvoidWrite class in sphinx/util/osutil.py to determine if there are any issues with the getvalue method and the encoding handling, and implement fixes if needed.","diff":"diff --git a/sphinx/util/osutil.py b/sphinx/util/osutil.py\nindex 53bffd9..60c7faf 100644\n--- a/sphinx/util/osutil.py\n+++ b/sphinx/util/osutil.py\n@@ -219,11 +219,16 @@ class FileAvoidWrite:\n     def close(self) -> None:\n         \"\"\"Stop accepting writes and write file, if needed.\"\"\"\n         if not self._io:\n-            raise Exception('FileAvoidWrite does not support empty files.')\n+            # Nothing was written, so nothing to do\n+            return\n \n         buf = self.getvalue()\n         self._io.close()\n \n+        # Don't write empty files\n+        if not buf:\n+            return\n+\n         try:\n             with open(self._path, encoding='utf-8') as old_f:\n                 old_content = old_f.read()\n@@ -242,6 +247,16 @@ class FileAvoidWrite:\n         self.close()\n         return True\n \n+    def getvalue(self) -> str:\n+        \"\"\"Return the current contents of the buffer as a string.\n+        \n+        This method is explicitly defined to handle the case when no data\n+        has been written yet, and to ensure consistent encoding behavior.\n+        \"\"\"\n+        if not self._io:\n+            return \"\"\n+        return self._io.getvalue()\n+        \n     def __getattr__(self, name: str) -> Any:\n         # Proxy to _io instance.\n         if not self._io:\ndiff --git a/tests/test_util_osutil_fileavoidwrite.py b/tests/test_util_osutil_fileavoidwrite.py\nnew file mode 100644\nindex 0000000..36c758e\n--- /dev/null\n+++ b/tests/test_util_osutil_fileavoidwrite.py\n@@ -0,0 +1,141 @@\n+\"\"\"\n+    test_util_osutil_fileavoidwrite\n+    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+    Tests sphinx.util.osutil.FileAvoidWrite class.\n+\n+    :copyright: Copyright 2007-2021 by the Sphinx team, see AUTHORS.\n+    :license: BSD, see LICENSE for details.\n+\"\"\"\n+\n+import os\n+from unittest import mock\n+\n+import pytest\n+\n+from sphinx.util.osutil import FileAvoidWrite\n+\n+\n+def test_fileavoidwrite_basic(tmpdir):\n+    \"\"\"Test basic functionality of FileAvoidWrite.\"\"\"\n+    filename = str(tmpdir / 'test.txt')\n+    \n+    # Write to a new file\n+    with FileAvoidWrite(filename) as f:\n+        f.write('Hello')\n+        f.write(' World')\n+    \n+    assert os.path.exists(filename)\n+    with open(filename, encoding='utf-8') as f:\n+        assert f.read() == 'Hello World'\n+    \n+    # Get file modification time\n+    mtime = os.path.getmtime(filename)\n+    \n+    # Write the same content again - file should not be modified\n+    with FileAvoidWrite(filename) as f:\n+        f.write('Hello World')\n+    \n+    assert os.path.getmtime(filename) == mtime  # File wasn't touched\n+    \n+    # Write different content - file should be modified\n+    with FileAvoidWrite(filename) as f:\n+        f.write('Different content')\n+    \n+    assert os.path.getmtime(filename) > mtime  # File was modified\n+    with open(filename, encoding='utf-8') as f:\n+        assert f.read() == 'Different content'\n+\n+\n+def test_fileavoidwrite_getvalue(tmpdir):\n+    \"\"\"Test the getvalue method of FileAvoidWrite.\"\"\"\n+    filename = str(tmpdir / 'test.txt')\n+    \n+    # Test getvalue with content\n+    with FileAvoidWrite(filename) as f:\n+        f.write('Hello')\n+        f.write(' World')\n+        # Test getvalue() returns the correct content\n+        assert f.getvalue() == 'Hello World'\n+    \n+    # Verify file was written correctly\n+    with open(filename, encoding='utf-8') as f:\n+        assert f.read() == 'Hello World'\n+        \n+    # Test getvalue with no content\n+    filename2 = str(tmpdir / 'test2.txt')\n+    with FileAvoidWrite(filename2) as f:\n+        # Test getvalue() returns empty string when no data written\n+        assert f.getvalue() == \"\"\n+    \n+    # No file should be created\n+    assert not os.path.exists(filename2)\n+\n+\n+def test_fileavoidwrite_encoding(tmpdir):\n+    \"\"\"Test encoding handling in FileAvoidWrite.\"\"\"\n+    filename = str(tmpdir / 'test.txt')\n+    \n+    # Test with non-ASCII characters\n+    test_string = 'Hello 世界'  # Hello World in Chinese\n+    \n+    with FileAvoidWrite(filename) as f:\n+        f.write(test_string)\n+    \n+    # Verify file was written with correct encoding\n+    with open(filename, encoding='utf-8') as f:\n+        assert f.read() == test_string\n+\n+\n+def test_fileavoidwrite_empty(tmpdir):\n+    \"\"\"Test handling of empty files.\"\"\"\n+    filename = str(tmpdir / 'test.txt')\n+    \n+    # FileAvoidWrite should handle empty files gracefully\n+    with FileAvoidWrite(filename) as f:\n+        pass  # No write operations\n+    \n+    # No file should be created when no content is written\n+    assert not os.path.exists(filename)\n+    \n+    # Empty content should also not create a file\n+    with FileAvoidWrite(filename) as f:\n+        f.write(\"\")\n+    \n+    assert not os.path.exists(filename)\n+\n+\n+def test_fileavoidwrite_methods(tmpdir):\n+    \"\"\"Test other methods inherited from StringIO.\"\"\"\n+    filename = str(tmpdir / 'test.txt')\n+    \n+    with FileAvoidWrite(filename) as f:\n+        f.write('Hello World')\n+        f.seek(0)\n+        assert f.read(5) == 'Hello'\n+        f.seek(6)\n+        assert f.read() == 'World'\n+        \n+        # Test that getvalue() works regardless of current position\n+        f.seek(0)\n+        assert f.getvalue() == 'Hello World'\n+        f.seek(6)\n+        assert f.getvalue() == 'Hello World'\n+\n+\n+def test_fileavoidwrite_exception_handling(tmpdir):\n+    \"\"\"Test exception handling in FileAvoidWrite.\"\"\"\n+    filename = str(tmpdir / 'test.txt')\n+    \n+    # Create a file with initial content\n+    with open(filename, 'w', encoding='utf-8') as f:\n+        f.write('Initial content')\n+    \n+    # Mock open to raise an exception when reading the file\n+    with mock.patch('builtins.open', side_effect=OSError):\n+        with FileAvoidWrite(filename) as f:\n+            f.write('New content')\n+    \n+    # File should be written even if reading the original fails\n+    with open(filename, encoding='utf-8') as f:\n+        assert f.read() == 'New content'\n\\ No newline at end of file"}]}]}